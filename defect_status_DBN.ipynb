{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dbn.tensorflow import SupervisedDBNClassification\n",
    "from dbn.tensorflow.models import UnsupervisedDBN\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv, re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.classification import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import svm, tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n",
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTokensOfOneFile( oneFileContent ):\n",
    "    stemmer_obj  = SnowballStemmer(\"english\")\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and\n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    \n",
    "    # Remove non-letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", oneFileContent)\n",
    "    \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # In Python, searching a set is much faster than searching a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # Only inlcude words at least of length 3\n",
    "    valid_len_words = [w for w in meaningful_words if len(w) >= 3]\n",
    "\n",
    "    # convert words to utf\n",
    "    stemmed_words = [stemmer_obj.stem(token) for token in valid_len_words]\n",
    "    \n",
    "    #Join the words back into one string separated by space, and return the result.\n",
    "    return( \" \".join( stemmed_words ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def giveFileContent(fileNameParam):\n",
    "    str2ret=\"\"\n",
    "    for line_ in open(\"..//\" + re.split('V5/', fileNameParam)[1], 'rU'):\n",
    "        li=line_.strip()\n",
    "        str2ret = str2ret + line_.rstrip()\n",
    "    return str2ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokensForTokenization(file_name):\n",
    "    completeCorpus    = [] ## a list of lists with tokens from defected and non defected files\n",
    "    for fileToRead in file_name:\n",
    "        fileContentAsStr = giveFileContent(fileToRead)\n",
    "        filtered_str_from_one_file = processTokensOfOneFile(fileContentAsStr)\n",
    "        completeCorpus.append(filtered_str_from_one_file)       \n",
    "    return completeCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_performance(true_label, predicted_label):   \n",
    "    precision = recall = f1 = np.zeros(2, dtype=np.float32)\n",
    "    report = classification_report(true_label, predicted_label, digits=3)\n",
    "    precision = precision_score(true_label, predicted_label, average=None, labels=[0,1])\n",
    "    recall = recall_score(true_label, predicted_label, average=None, labels=[0,1])\n",
    "    f1 = f1_score(true_label, predicted_label, average=None, labels=[0,1])\n",
    "    return recall, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLabelEncoder(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
    "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
    "        \"\"\"\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        # self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "    def fit(self, data_list):\n",
    "        \"\"\"\n",
    "        This will fit the encoder for all the unique values and introduce unknown value\n",
    "        :param data_list: A list of string\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_list):\n",
    "        \"\"\"\n",
    "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
    "        :param data_list:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        new_data_list = list(data_list)\n",
    "        for unique_item in np.unique(data_list):\n",
    "            if unique_item not in self.label_encoder.classes_:\n",
    "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
    "\n",
    "        return self.label_encoder.transform(new_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_source_code(file_name):     \n",
    "    unfilteredTokensFromFile = getTokensForTokenization(file_name)\n",
    "    return unfilteredTokensFromFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_vector_of_nodes(docs):\n",
    "    #documents\n",
    "    #docs = ['if foo for bar car', 'foo for if bar']\n",
    "    \n",
    "    docs = [x.lower() for x in docs]\n",
    "  \n",
    "    # split documents to tokens\n",
    "    words = [doc.split(\" \") for doc in docs]\n",
    "    #print(\"All tokens: \", words)\n",
    "    \n",
    "    # find the length of vectors\n",
    "    max_len = np.max([len(x) for x in words])\n",
    "    #print(\"Vector Length: \", max_len)\n",
    "    \n",
    "    # convert list of of token-lists to one flat list of tokens\n",
    "    flatten_words = list(itertools.chain.from_iterable(words))\n",
    "    #print(\"Flatten tokens: \", flatten_words)\n",
    "    \n",
    "    #fine all the unique tokens\n",
    "    unique_words = np.unique(flatten_words)\n",
    "    #print(\"Unique tokens: \", unique_words)\n",
    "    print(\"Feature Number: \", unique_words.size)\n",
    "    \n",
    "    # integer encode\n",
    "    encoded_docs = []\n",
    "    label_encoder = MyLabelEncoder()\n",
    "    label_encoder.fit(unique_words)\n",
    "    for doc in docs:\n",
    "        #print(doc)\n",
    "        words = doc.split(\" \")\n",
    "        #print(words)\n",
    "        integer_encoded = label_encoder.transform(words)\n",
    "        integer_encoded = np.pad(integer_encoded, (0, max_len - len(integer_encoded))) #padding with 0 to make fixed sized vectors\n",
    "        #print(integer_encoded)\n",
    "        encoded_docs.append(integer_encoded)\n",
    "    \n",
    "    #print(encoded_docs)\n",
    "    return encoded_docs, unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_PCA(train, test):\n",
    "    # Since PCA is effected by scale, we need to scale the features in the data before applying PCA\n",
    "    scaler = StandardScaler()\n",
    "    # Fit on training set only.\n",
    "    scaler.fit(train)\n",
    "    # Apply transform to both the training set and the test set.\n",
    "    train = scaler.transform(train)\n",
    "    test = scaler.transform(test)\n",
    "\n",
    "    # Make an instance of the Model\n",
    "    pca = PCA(0.95) #  choose the minimum number of principal components such that 95% of the variance is retained.\n",
    "    # We are fitting PCA on the training set only.\n",
    "    pca.fit(train)\n",
    "    print (\"Number of selected components: \", pca.n_components_)\n",
    "    #print (pd.DataFrame(pca.components_))\n",
    "    \n",
    "    # Apply the mapping (transform) to both the training set and the test set\n",
    "    print(\"Before applying PCA train set size: \", train.shape)\n",
    "    print(\"Before applying PCA test set size: \", test.shape)\n",
    "    train = pca.transform(train)\n",
    "    test = pca.transform(test)\n",
    "    print(\"After applying PCA train set size: \", train.shape)\n",
    "    print(\"After applying PCA test set size: \", test.shape)\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_classifier(clf, feature_list):\n",
    "    dbn = UnsupervisedDBN(hidden_layers_structure=[feature_list.size, feature_list.size],\n",
    "                              batch_size=64,\n",
    "                              learning_rate_rbm=0.06,\n",
    "                              n_epochs_rbm=1,\n",
    "                              activation_function='sigmoid',\n",
    "                              verbose =0)\n",
    "\n",
    "    classifier = Pipeline(steps=[('dbn', dbn), ('clf', clf)], verbose= True)  \n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_cv(data, true_label, feature_list):\n",
    "    \n",
    "    data = np.array(data)\n",
    "    \n",
    "    # 10 fold cv\n",
    "    kf = KFold(n_splits=2, shuffle = True, random_state = 7)\n",
    "\n",
    "    cv_recall_DT = []\n",
    "    cv_precision_DT = []\n",
    "    cv_f1_DT = []\n",
    "    cv_fit_time_DT = []\n",
    "    cv_predict_time_DT = []\n",
    "    \n",
    "    cv_recall_KNN = []\n",
    "    cv_precision_KNN = []\n",
    "    cv_f1_KNN = []\n",
    "    cv_fit_time_KNN = []\n",
    "    cv_predict_time_KNN = []\n",
    "    \n",
    "    cv_recall_SVM = []\n",
    "    cv_precision_SVM = []\n",
    "    cv_f1_SVM = []\n",
    "    cv_fit_time_SVM = []\n",
    "    cv_predict_time_SVM = []\n",
    "    \n",
    "    cv_recall_NB = []\n",
    "    cv_precision_NB = []\n",
    "    cv_f1_NB = []\n",
    "    cv_fit_time_NB = []\n",
    "    cv_predict_time_NB = []\n",
    "    \n",
    "    cv_recall_RF = []\n",
    "    cv_precision_RF = []\n",
    "    cv_f1_RF = []\n",
    "    cv_fit_time_RF =[]\n",
    "    cv_predict_time_RF = []\n",
    "\n",
    "\n",
    "    for train_index, test_index in kf.split(data):        \n",
    "        train, test = data[train_index], data[test_index]\n",
    "        train_label, test_label = true_label[train_index], true_label[test_index]\n",
    "        \n",
    "        train, test = apply_PCA(train, test)\n",
    "        \n",
    "        clf = tree.DecisionTreeClassifier(criterion='gini', splitter='best', \n",
    "                                      min_samples_split = 2, min_weight_fraction_leaf=0.0)\n",
    "        classifier = semantic_classifier(clf, feature_list)\n",
    "        start = time.perf_counter()\n",
    "        classifier.fit(train, train_label)\n",
    "        end = time.perf_counter()\n",
    "        fit_time = end - start\n",
    "        start = time.perf_counter()\n",
    "        predicted_label = classifier.predict(test)\n",
    "        end = time.perf_counter()\n",
    "        predict_time = end - start\n",
    "        recall, precision, f1 = measure_performance(test_label, predicted_label)\n",
    "        cv_recall_DT.append(recall)\n",
    "        cv_precision_DT.append(precision)\n",
    "        cv_f1_DT.append(f1)\n",
    "        cv_fit_time_DT.append(fit_time)\n",
    "        cv_predict_time_DT.append(predict_time)  \n",
    "        print(\"----------------CART------------------\")\n",
    "        CART_features = eli5.explain_weights_df(classifier.named_steps['clf'], top=50, feature_names = feature_list)\n",
    "        print(CART_features)      \n",
    "        print(\"*\"*100)\n",
    "\n",
    "        \n",
    "        clf = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "        classifier = semantic_classifier(clf, feature_list)\n",
    "        start = time.perf_counter()\n",
    "        classifier.fit(train, train_label)\n",
    "        end = time.perf_counter()\n",
    "        fit_time = end - start\n",
    "        start = time.perf_counter()\n",
    "        predicted_label = classifier.predict(test)\n",
    "        end = time.perf_counter()\n",
    "        predict_time = end - start\n",
    "        recall, precision, f1 = measure_performance(test_label, predicted_label)\n",
    "        cv_recall_KNN.append(recall)\n",
    "        cv_precision_KNN.append(precision)\n",
    "        cv_f1_KNN.append(f1)\n",
    "        cv_fit_time_KNN.append(fit_time)\n",
    "        cv_predict_time_KNN.append(predict_time)\n",
    "        print(\"----------------KNN------------------\")\n",
    "        KNN_features = eli5.explain_weights_df(classifier.named_steps['clf'], top=50, feature_names = feature_list)\n",
    "        print(KNN_features)      \n",
    "        print(\"*\"*100)\n",
    "\n",
    "        \n",
    "        clf = svm.SVC(gamma='auto', C = 20.0, kernel='rbf')\n",
    "        classifier = semantic_classifier(clf, feature_list)\n",
    "        start = time.perf_counter()\n",
    "        classifier.fit(train, train_label)\n",
    "        end = time.perf_counter()\n",
    "        fit_time = end - start\n",
    "        start = time.perf_counter()\n",
    "        predicted_label = classifier.predict(test)\n",
    "        end = time.perf_counter()\n",
    "        predict_time = end - start\n",
    "        recall, precision, f1 = measure_performance(test_label, predicted_label)\n",
    "        cv_recall_SVM.append(recall)\n",
    "        cv_precision_SVM.append(precision)\n",
    "        cv_f1_SVM.append(f1)\n",
    "        cv_fit_time_SVM.append(fit_time)\n",
    "        cv_predict_time_SVM.append(predict_time)\n",
    "        print(\"----------------SVM------------------\")\n",
    "        SVM_features = eli5.explain_weights_df(classifier.named_steps['clf'], top=50, feature_names = feature_list)\n",
    "        print(SVM_features)      \n",
    "        print(\"*\"*100)\n",
    "        \n",
    "        \n",
    "        clf = GaussianNB()\n",
    "        classifier = semantic_classifier(clf, feature_list)\n",
    "        start = time.perf_counter()\n",
    "        classifier.fit(train, train_label)\n",
    "        end = time.perf_counter()\n",
    "        fit_time = end - start\n",
    "        start = time.perf_counter()\n",
    "        predicted_label = classifier.predict(test)\n",
    "        end = time.perf_counter()\n",
    "        predict_time = end - start\n",
    "        recall, precision, f1 = measure_performance(test_label, predicted_label)\n",
    "        cv_recall_NB.append(recall)\n",
    "        cv_precision_NB.append(precision)\n",
    "        cv_f1_NB.append(f1)\n",
    "        cv_fit_time_NB.append(fit_time)\n",
    "        cv_predict_time_NB.append(predict_time)\n",
    "        print(\"----------------NB------------------\")\n",
    "        NB_features = eli5.explain_weights_df(classifier.named_steps['clf'], top=50, feature_names = feature_list)\n",
    "        print(NB_features)      \n",
    "        print(\"*\"*100)      \n",
    "        \n",
    "        \n",
    "        clf = RandomForestClassifier(n_estimators=10, criterion='gini')\n",
    "        classifier = semantic_classifier(clf, feature_list)\n",
    "        start = time.perf_counter()\n",
    "        classifier.fit(train, train_label)\n",
    "        end = time.perf_counter()\n",
    "        fit_time = end - start\n",
    "        start = time.perf_counter()\n",
    "        predicted_label = classifier.predict(test)\n",
    "        end = time.perf_counter()\n",
    "        predict_time = end - start\n",
    "        recall, precision, f1 = measure_performance(test_label, predicted_label)\n",
    "        cv_recall_RF.append(recall)\n",
    "        cv_precision_RF.append(precision)\n",
    "        cv_f1_RF.append(f1)\n",
    "        cv_fit_time_RF.append(fit_time)\n",
    "        cv_predict_time_RF.append(predict_time)\n",
    "        print(\"----------------RF------------------\")\n",
    "        RF_features = eli5.explain_weights_df(classifier.named_steps['clf'], top=50, feature_names = feature_list)\n",
    "        print(RF_features)      \n",
    "        print(\"*\"*100)\n",
    "\n",
    "        \n",
    "    recall_DT = np.mean(cv_recall_DT, axis= 0)\n",
    "    precision_DT = np.mean(cv_precision_DT, axis= 0)\n",
    "    f1_DT = np.mean(cv_f1_DT, axis= 0)\n",
    "    fit_time_DT = np.mean(cv_fit_time_DT)\n",
    "    predict_time_DT = np.mean(cv_predict_time_DT)\n",
    "\n",
    "    recall_KNN = np.mean(cv_recall_KNN, axis= 0)\n",
    "    precision_KNN = np.mean(cv_precision_KNN, axis= 0)\n",
    "    f1_KNN = np.mean(cv_f1_KNN, axis= 0)\n",
    "    fit_time_KNN = np.mean(cv_fit_time_KNN)\n",
    "    predict_time_KNN = np.mean(cv_predict_time_KNN)\n",
    "    \n",
    "    recall_SVM = np.mean(cv_recall_SVM, axis= 0)\n",
    "    precision_SVM = np.mean(cv_precision_SVM, axis= 0)\n",
    "    f1_SVM =  np.mean(cv_f1_SVM, axis= 0)\n",
    "    fit_time_SVM = np.mean(cv_fit_time_SVM)\n",
    "    predict_time_SVM = np.mean(cv_predict_time_SVM)\n",
    "    \n",
    "    recall_NB = np.mean(cv_recall_NB, axis= 0)\n",
    "    precision_NB = np.mean(cv_precision_NB, axis= 0)\n",
    "    f1_NB = np.mean(cv_f1_NB, axis= 0)\n",
    "    fit_time_NB = np.mean(cv_fit_time_NB)\n",
    "    predict_time_NB = np.mean(cv_predict_time_NB)\n",
    "    \n",
    "    recall_RF = np.mean(cv_recall_RF, axis= 0)\n",
    "    precision_RF = np.mean(cv_precision_RF, axis= 0)\n",
    "    f1_RF = np.mean(cv_f1_RF, axis= 0)\n",
    "    fit_time_RF = np.mean(cv_fit_time_RF)\n",
    "    predict_time_RF = np.mean(cv_predict_time_RF)\n",
    "    \n",
    "    return recall_DT, precision_DT, f1_DT, fit_time_DT, predict_time_DT, recall_KNN, precision_KNN, f1_KNN,\\\n",
    "    fit_time_KNN, predict_time_KNN, recall_SVM, precision_SVM, f1_SVM, fit_time_SVM, predict_time_SVM, recall_NB,\\\n",
    "    precision_NB, f1_NB, fit_time_NB, predict_time_NB, recall_RF, precision_RF, f1_RF, fit_time_RF, predict_time_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeated_test(data, true_label, unique_words):\n",
    "    repeated_recall_DT = []\n",
    "    repeated_precision_DT = []\n",
    "    repeated_f1_DT = []\n",
    "    repeated_fit_time_DT = []\n",
    "    repeated_predict_time_DT = []\n",
    "    \n",
    "    repeated_recall_KNN = []\n",
    "    repeated_precision_KNN = []\n",
    "    repeated_f1_KNN = []\n",
    "    repeated_fit_time_KNN = []\n",
    "    repeated_predict_time_KNN = []\n",
    "    \n",
    "    repeated_recall_SVM = []\n",
    "    repeated_precision_SVM = []\n",
    "    repeated_f1_SVM = []\n",
    "    repeated_fit_time_SVM = []\n",
    "    repeated_predict_time_SVM = []\n",
    "    \n",
    "    repeated_recall_NB = []\n",
    "    repeated_precision_NB = []\n",
    "    repeated_f1_NB = []\n",
    "    repeated_fit_time_NB = []\n",
    "    repeated_predict_time_NB = []\n",
    "    \n",
    "    repeated_recall_RF = []\n",
    "    repeated_precision_RF = []\n",
    "    repeated_f1_RF = []\n",
    "    repeated_fit_time_RF = []\n",
    "    repeated_predict_time_RF = []\n",
    "    \n",
    "    recall_DT= precision_DT= f1_DT= fit_time_DT= predict_time_DT= recall_KNN= precision_KNN= f1_KNN=\\\n",
    "    fit_time_KNN= predict_time_KNN= recall_SVM = precision_SVM= f1_SVM= fit_time_SVM= predict_time_SVM\\\n",
    "    = recall_NB= precision_NB= f1_NB= fit_time_NB= predict_time_NB= recall_RF= precision_RF= f1_RF\\\n",
    "    = fit_time_RF= predict_time_RF = 0\n",
    "    \n",
    "    for i in range(1):\n",
    "        recall_DT, precision_DT, f1_DT, fit_time_DT, predict_time_DT, recall_KNN, precision_KNN, f1_KNN,\\\n",
    "        fit_time_KNN, predict_time_KNN, recall_SVM, precision_SVM, f1_SVM, fit_time_SVM, predict_time_SVM,\\\n",
    "        recall_NB, precision_NB, f1_NB, fit_time_NB, predict_time_NB, recall_RF, precision_RF, f1_RF, fit_time_RF,\\\n",
    "        predict_time_RF = kfold_cv(data, true_label, unique_words)\n",
    "        \n",
    "        repeated_recall_DT.append(recall_DT)\n",
    "        repeated_precision_DT.append(precision_DT)\n",
    "        repeated_f1_DT.append(f1_DT)\n",
    "        repeated_fit_time_DT.append(fit_time_DT) \n",
    "        repeated_predict_time_DT.append(predict_time_DT)\n",
    "\n",
    "        repeated_recall_KNN.append(recall_KNN)\n",
    "        repeated_precision_KNN.append(precision_KNN)\n",
    "        repeated_f1_KNN.append(f1_KNN)\n",
    "        repeated_fit_time_KNN.append(fit_time_KNN) \n",
    "        repeated_predict_time_KNN.append(predict_time_KNN)\n",
    "\n",
    "        repeated_recall_SVM.append(recall_SVM)\n",
    "        repeated_precision_SVM.append(precision_SVM)\n",
    "        repeated_f1_SVM.append(f1_SVM)\n",
    "        repeated_fit_time_SVM.append(fit_time_SVM) \n",
    "        repeated_predict_time_SVM.append(predict_time_SVM)\n",
    "        \n",
    "        repeated_recall_NB.append(recall_NB)\n",
    "        repeated_precision_NB.append(precision_NB)\n",
    "        repeated_f1_NB.append(f1_NB)\n",
    "        repeated_fit_time_NB.append(fit_time_NB) \n",
    "        repeated_predict_time_NB.append(predict_time_NB)\n",
    "        \n",
    "        repeated_recall_RF.append(recall_RF)\n",
    "        repeated_precision_RF.append(precision_RF)\n",
    "        repeated_f1_RF.append(f1_RF)\n",
    "        repeated_fit_time_RF.append(fit_time_RF) \n",
    "        repeated_predict_time_RF.append(predict_time_RF)\n",
    "        \n",
    "    print(\"-------DT-------\")\n",
    "    print(\"Recall:\", np.median(repeated_recall_DT, axis= 0))\n",
    "    print(\"Precision:\", np.median(repeated_precision_DT, axis= 0))\n",
    "    print(\"f1 score:\", np.median(repeated_f1_DT, axis= 0))\n",
    "    print(\"Fit time:\", np.median(repeated_fit_time_DT))\n",
    "    print(\"Predict time:\", np.median(repeated_predict_time_DT))\n",
    "\n",
    "    print(\"-------KNN-------\")\n",
    "    print(\"Recall:\", np.median(repeated_recall_KNN, axis= 0))\n",
    "    print(\"Precision:\", np.median(repeated_precision_KNN, axis= 0))\n",
    "    print(\"f1 score:\", np.median(repeated_f1_KNN, axis= 0))\n",
    "    print(\"Fit time:\", np.median(repeated_fit_time_KNN))\n",
    "    print(\"Predict time:\", np.median(repeated_predict_time_KNN))\n",
    "\n",
    "    print(\"-------SVM-------\")\n",
    "    print(\"Recall:\", np.median(repeated_recall_SVM, axis= 0))\n",
    "    print(\"Precision:\", np.median(repeated_precision_SVM, axis= 0))\n",
    "    print(\"f1 score:\", np.median(repeated_f1_SVM, axis= 0))\n",
    "    print(\"Fit time:\", np.median(repeated_fit_time_SVM))\n",
    "    print(\"Predict time:\", np.median(repeated_predict_time_SVM))\n",
    "    \n",
    "    print(\"-------NB-------\")\n",
    "    print(\"Recall:\", np.median(repeated_recall_NB, axis= 0))\n",
    "    print(\"Precision:\", np.median(repeated_precision_NB, axis= 0))\n",
    "    print(\"f1 score:\", np.median(repeated_f1_NB, axis= 0))\n",
    "    print(\"Fit time:\", np.median(repeated_fit_time_NB))\n",
    "    print(\"Predict time:\", np.median(repeated_predict_time_NB))\n",
    "    \n",
    "    print(\"-------RF-------\")\n",
    "    print(\"Recall:\", np.median(repeated_recall_RF, axis= 0))\n",
    "    print(\"Precision:\", np.median(repeated_precision_RF, axis= 0))\n",
    "    print(\"f1 score:\", np.median(repeated_f1_RF, axis= 0))\n",
    "    print(\"Fit time:\", np.median(repeated_fit_time_RF))\n",
    "    print(\"Predict time:\", np.median(repeated_predict_time_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial process data shape:  (43, 12)\n",
      "Initial code data shape:  (6396, 10)\n",
      "Feature Number:  1201\n",
      "Number of selected components:  6\n",
      "Before applying PCA train set size:  (13, 1770)\n",
      "Before applying PCA test set size:  (14, 1770)\n",
      "After applying PCA train set size:  (13, 6)\n",
      "After applying PCA test set size:  (14, 6)\n",
      "[Pipeline] ............... (step 1 of 2) Processing dbn, total=   1.8s\n",
      "[Pipeline] ............... (step 2 of 2) Processing clf, total=   0.0s\n",
      "----------------CART------------------\n",
      "                       feature  weight\n",
      "0                callablebyown     1.0\n",
      "1                       getter     0.0\n",
      "2                         flag     0.0\n",
      "3                          fix     0.0\n",
      "4                          fit     0.0\n",
      "5                   firstblood     0.0\n",
      "6                        first     0.0\n",
      "7                         firm     0.0\n",
      "8                         fire     0.0\n",
      "9                        finir     0.0\n",
      "10                        find     0.0\n",
      "11                       final     0.0\n",
      "12                        file     0.0\n",
      "13                        fifo     0.0\n",
      "14                        feel     0.0\n",
      "15                      featur     0.0\n",
      "16                         fcc     0.0\n",
      "17                  favoritism     0.0\n",
      "18                        faut     0.0\n",
      "19                         faq     0.0\n",
      "20                        fals     0.0\n",
      "21                    fallback     0.0\n",
      "22                      follow     0.0\n",
      "23                    fonction     0.0\n",
      "24                      forget     0.0\n",
      "25                       found     0.0\n",
      "26               getmaxemprunt     0.0\n",
      "27  getmindailywithdrawallimit     0.0\n",
      "28                   fundmanag     0.0\n",
      "29                   getdemand     0.0\n",
      "30             getlastwithdraw     0.0\n",
      "31          getiscontractvalid     0.0\n",
      "32     getinitialwithdrawaldon     0.0\n",
      "33          getinitialwithdraw     0.0\n",
      "34    gethashoftheproposaldocu     0.0\n",
      "35                 getetherbal     0.0\n",
      "36                        fait     0.0\n",
      "37                       futur     0.0\n",
      "38                        fair     0.0\n",
      "39                        fund     0.0\n",
      "40                    function     0.0\n",
      "41                       fulli     0.0\n",
      "42                       front     0.0\n",
      "43                   fromblock     0.0\n",
      "44                      freeli     0.0\n",
      "45                        free     0.0\n",
      "46                     founder     0.0\n",
      "47                     foundat     0.0\n",
      "48                      gettim     0.0\n",
      "49                         zrx     0.0\n",
      "****************************************************************************************************\n",
      "[Pipeline] ............... (step 1 of 2) Processing dbn, total=   1.9s\n",
      "[Pipeline] ............... (step 2 of 2) Processing clf, total=   0.0s\n",
      "----------------KNN------------------\n",
      "None\n",
      "****************************************************************************************************\n",
      "[Pipeline] ............... (step 1 of 2) Processing dbn, total=   2.1s\n",
      "[Pipeline] ............... (step 2 of 2) Processing clf, total=   0.0s\n",
      "----------------SVM------------------\n",
      "None\n",
      "****************************************************************************************************\n",
      "[Pipeline] ............... (step 1 of 2) Processing dbn, total=   2.0s\n",
      "[Pipeline] ............... (step 2 of 2) Processing clf, total=   0.0s\n",
      "----------------NB------------------\n",
      "None\n",
      "****************************************************************************************************\n",
      "[Pipeline] ............... (step 1 of 2) Processing dbn, total=   2.1s\n",
      "[Pipeline] ............... (step 2 of 2) Processing clf, total=   0.0s\n",
      "----------------RF------------------\n",
      "                       feature    weight     std\n",
      "0              previousorderid  0.125000  0.3000\n",
      "1   getmindailywithdrawallimit  0.125000  0.3000\n",
      "2                        clone  0.125000  0.3000\n",
      "3                         pile  0.125000  0.3000\n",
      "4                        impli  0.125000  0.3000\n",
      "5                        final  0.084375  0.2025\n",
      "6                newclonetoken  0.084375  0.2025\n",
      "7                changecontrol  0.067708  0.1625\n",
      "8                         open  0.057292  0.1375\n",
      "9                        check  0.040625  0.0975\n",
      "10                      gettim  0.040625  0.0975\n",
      "11                        fifo  0.000000  0.0000\n",
      "12                      featur  0.000000  0.0000\n",
      "13                        file  0.000000  0.0000\n",
      "14                         fcc  0.000000  0.0000\n",
      "15                  favoritism  0.000000  0.0000\n",
      "16                        faut  0.000000  0.0000\n",
      "17                        find  0.000000  0.0000\n",
      "18                       finir  0.000000  0.0000\n",
      "19                        fire  0.000000  0.0000\n",
      "20                        firm  0.000000  0.0000\n",
      "21                         faq  0.000000  0.0000\n",
      "22                       first  0.000000  0.0000\n",
      "23                        feel  0.000000  0.0000\n",
      "24                  firstblood  0.000000  0.0000\n",
      "25                        fund  0.000000  0.0000\n",
      "26                         fix  0.000000  0.0000\n",
      "27                   getclient  0.000000  0.0000\n",
      "28    gethashoftheproposaldocu  0.000000  0.0000\n",
      "29                 getetherbal  0.000000  0.0000\n",
      "30                   getdemand  0.000000  0.0000\n",
      "31           getdateofsignatur  0.000000  0.0000\n",
      "32     getdailywithdrawallimit  0.000000  0.0000\n",
      "33               getcontractor  0.000000  0.0000\n",
      "34                    fallback  0.000000  0.0000\n",
      "35                         zrx  0.000000  0.0000\n",
      "36                       fulli  0.000000  0.0000\n",
      "37                       front  0.000000  0.0000\n",
      "38                   fromblock  0.000000  0.0000\n",
      "39                      freeli  0.000000  0.0000\n",
      "40                        free  0.000000  0.0000\n",
      "41                     founder  0.000000  0.0000\n",
      "42                     foundat  0.000000  0.0000\n",
      "43                       found  0.000000  0.0000\n",
      "44                      forget  0.000000  0.0000\n",
      "45                    fonction  0.000000  0.0000\n",
      "46                      follow  0.000000  0.0000\n",
      "47                        flag  0.000000  0.0000\n",
      "48                        fals  0.000000  0.0000\n",
      "49                         fit  0.000000  0.0000\n",
      "****************************************************************************************************\n",
      "Number of selected components:  6\n",
      "Before applying PCA train set size:  (14, 1770)\n",
      "Before applying PCA test set size:  (13, 1770)\n",
      "After applying PCA train set size:  (14, 6)\n",
      "After applying PCA test set size:  (13, 6)\n",
      "[Pipeline] ............... (step 1 of 2) Processing dbn, total=   1.9s\n",
      "[Pipeline] ............... (step 2 of 2) Processing clf, total=   0.0s\n",
      "----------------CART------------------\n",
      "                     feature  weight\n",
      "0                       voir     1.0\n",
      "1         getiscontractvalid     0.0\n",
      "2                        fix     0.0\n",
      "3                        fit     0.0\n",
      "4                 firstblood     0.0\n",
      "5                      first     0.0\n",
      "6                       firm     0.0\n",
      "7                       fire     0.0\n",
      "8                      finir     0.0\n",
      "9                       find     0.0\n",
      "10                     final     0.0\n",
      "11                      flag     0.0\n",
      "12                      file     0.0\n",
      "13                      feel     0.0\n",
      "14                    featur     0.0\n",
      "15                       fcc     0.0\n",
      "16                favoritism     0.0\n",
      "17                      faut     0.0\n",
      "18                       faq     0.0\n",
      "19                      fals     0.0\n",
      "20                  fallback     0.0\n",
      "21                      fait     0.0\n",
      "22                      fair     0.0\n",
      "23                      fifo     0.0\n",
      "24                    follow     0.0\n",
      "25                  fonction     0.0\n",
      "26                    forget     0.0\n",
      "27           getmemberscount     0.0\n",
      "28                  function     0.0\n",
      "29           getlastwithdraw     0.0\n",
      "30   getdailywithdrawallimit     0.0\n",
      "31   getinitialwithdrawaldon     0.0\n",
      "32        getinitialwithdraw     0.0\n",
      "33  gethashoftheproposaldocu     0.0\n",
      "34               getetherbal     0.0\n",
      "35                 getdemand     0.0\n",
      "36         getdateofsignatur     0.0\n",
      "37                 fundmanag     0.0\n",
      "38                    getter     0.0\n",
      "39                   factori     0.0\n",
      "40                     fulli     0.0\n",
      "41                     front     0.0\n",
      "42                 fromblock     0.0\n",
      "43                    freeli     0.0\n",
      "44                      free     0.0\n",
      "45                   founder     0.0\n",
      "46                   foundat     0.0\n",
      "47                     found     0.0\n",
      "48                      fail     0.0\n",
      "49                       zrx     0.0\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 1 of 2) Processing dbn, total=   2.0s\n",
      "[Pipeline] ............... (step 2 of 2) Processing clf, total=   0.0s\n",
      "----------------KNN------------------\n",
      "None\n",
      "****************************************************************************************************\n",
      "[Pipeline] ............... (step 1 of 2) Processing dbn, total=   2.1s\n",
      "[Pipeline] ............... (step 2 of 2) Processing clf, total=   0.0s\n",
      "----------------SVM------------------\n",
      "None\n",
      "****************************************************************************************************\n",
      "[Pipeline] ............... (step 1 of 2) Processing dbn, total=   2.5s\n",
      "[Pipeline] ............... (step 2 of 2) Processing clf, total=   0.0s\n",
      "----------------NB------------------\n",
      "None\n",
      "****************************************************************************************************\n",
      "[Pipeline] ............... (step 1 of 2) Processing dbn, total=   2.3s\n",
      "[Pipeline] ............... (step 2 of 2) Processing clf, total=   0.0s\n",
      "----------------RF------------------\n",
      "                     feature  weight  std\n",
      "0                    everyth     0.1  0.3\n",
      "1                      input     0.1  0.3\n",
      "2                     thegnu     0.1  0.3\n",
      "3                   tokenbal     0.1  0.3\n",
      "4                     someth     0.1  0.3\n",
      "5                recoverpric     0.1  0.3\n",
      "6                      begin     0.1  0.3\n",
      "7                  gralement     0.1  0.3\n",
      "8                     extern     0.1  0.3\n",
      "9                     around     0.1  0.3\n",
      "10                      fals     0.0  0.0\n",
      "11                     first     0.0  0.0\n",
      "12                      firm     0.0  0.0\n",
      "13                       faq     0.0  0.0\n",
      "14                      fire     0.0  0.0\n",
      "15                     finir     0.0  0.0\n",
      "16                      faut     0.0  0.0\n",
      "17                    featur     0.0  0.0\n",
      "18                      find     0.0  0.0\n",
      "19                     final     0.0  0.0\n",
      "20                      file     0.0  0.0\n",
      "21                firstblood     0.0  0.0\n",
      "22                      feel     0.0  0.0\n",
      "23                favoritism     0.0  0.0\n",
      "24                       fcc     0.0  0.0\n",
      "25                      fifo     0.0  0.0\n",
      "26                 getdemand     0.0  0.0\n",
      "27                  fallback     0.0  0.0\n",
      "28   getdailywithdrawallimit     0.0  0.0\n",
      "29             getcontractor     0.0  0.0\n",
      "30                 getclient     0.0  0.0\n",
      "31         getdateofsignatur     0.0  0.0\n",
      "32  gethashoftheproposaldocu     0.0  0.0\n",
      "33               getetherbal     0.0  0.0\n",
      "34                      fait     0.0  0.0\n",
      "35                      fair     0.0  0.0\n",
      "36                     front     0.0  0.0\n",
      "37                       fit     0.0  0.0\n",
      "38                 fromblock     0.0  0.0\n",
      "39                      free     0.0  0.0\n",
      "40                   founder     0.0  0.0\n",
      "41                   foundat     0.0  0.0\n",
      "42                     found     0.0  0.0\n",
      "43                    forget     0.0  0.0\n",
      "44                  fonction     0.0  0.0\n",
      "45                    follow     0.0  0.0\n",
      "46                      flag     0.0  0.0\n",
      "47                       fix     0.0  0.0\n",
      "48                    freeli     0.0  0.0\n",
      "49                       zrx     0.0  0.0\n",
      "****************************************************************************************************\n",
      "-------DT-------\n",
      "Recall: [0.86363636 0.        ]\n",
      "Precision: [0.75714286 0.        ]\n",
      "f1 score: [0.79761905 0.        ]\n",
      "Fit time: 1.8747470785001497\n",
      "Predict time: 0.002783265499601839\n",
      "-------KNN-------\n",
      "Recall: [0.86363636 0.        ]\n",
      "Precision: [0.75714286 0.        ]\n",
      "f1 score: [0.79761905 0.        ]\n",
      "Fit time: 1.947986039999705\n",
      "Predict time: 0.00393694849935855\n",
      "-------SVM-------\n",
      "Recall: [1. 0.]\n",
      "Precision: [0.78021978 0.        ]\n",
      "f1 score: [0.875 0.   ]\n",
      "Fit time: 2.121656768998946\n",
      "Predict time: 0.0032289099999616155\n",
      "-------NB-------\n",
      "Recall: [1. 0.]\n",
      "Precision: [0.78021978 0.        ]\n",
      "f1 score: [0.875 0.   ]\n",
      "Fit time: 2.2458106029998817\n",
      "Predict time: 0.003477351499896031\n",
      "-------RF-------\n",
      "Recall: [0.86363636 0.        ]\n",
      "Precision: [0.75714286 0.        ]\n",
      "f1 score: [0.79761905 0.        ]\n",
      "Fit time: 2.2040629740013173\n",
      "Predict time: 0.00393394750153675\n"
     ]
    }
   ],
   "source": [
    "#process_data = pd.read_csv('..//FINAL_PROCESS_METRICS.csv') \n",
    "process_data = pd.read_csv('..//test.csv') \n",
    "print(\"Initial process data shape: \", process_data.shape)\n",
    "code_data = pd.read_csv('..//FINAL_CODE_METRICS.csv') \n",
    "print(\"Initial code data shape: \", code_data.shape)\n",
    "\n",
    "actual_process_file_name = process_data['file_']\n",
    "actual_code_file_name = code_data['FILE_PATH']\n",
    "\n",
    "formatted_process_file_name = []\n",
    "formatted_code_file_name = []   \n",
    "\n",
    "for item in actual_process_file_name:\n",
    "    formatted_process_file_name.append(re.split('V5/', item)[1]) \n",
    "for item in actual_code_file_name:\n",
    "    formatted_code_file_name.append(re.split('V5/', item)[1])\n",
    "    \n",
    "process_data['file_'] =  formatted_process_file_name   \n",
    "code_data['FILE_PATH'] =  formatted_code_file_name   \n",
    "    \n",
    "formatted_process_file_name = set(line.strip() for line in formatted_process_file_name)\n",
    "formatted_code_file_name = set(line.strip() for line in formatted_code_file_name)    \n",
    "    \n",
    "common_file_name = []\n",
    "true_label = []\n",
    "for common_entry in formatted_process_file_name & formatted_code_file_name:\n",
    "    if common_entry:\n",
    "        process_index =  process_data[process_data['file_'] == common_entry].index[0]\n",
    "        common_file_name.append(process_data.iloc[process_index]['FILE_PATH'])\n",
    "        true_label.append(process_data.iloc[process_index]['defect_status'])\n",
    "\n",
    "true_label = np.array(true_label)\n",
    "file_name = np.array(common_file_name)\n",
    "\n",
    "parsed_files = parse_source_code(file_name)\n",
    "encoded_files, unique_words = encode_vector_of_nodes(parsed_files)\n",
    "\n",
    "repeated_test(encoded_files, true_label, unique_words) #repeat kfold 10 times and report avarage performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
